{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **INCLUDE LIBRARY**\n",
    "\n",
    "import several necessary libraries to work with the data before doing analysis and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-02T12:31:00.693817Z",
     "iopub.status.busy": "2022-06-02T12:31:00.693299Z",
     "iopub.status.idle": "2022-06-02T12:31:08.890663Z",
     "shell.execute_reply": "2022-06-02T12:31:08.889532Z",
     "shell.execute_reply.started": "2022-06-02T12:31:00.693727Z"
    }
   },
   "outputs": [],
   "source": [
    "#Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#NN Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#Evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AUGMENTATION**\n",
    "\n",
    "We apply on-the-fly data augmentation, a technique to expand the training dataset size by creating a modified version of the original image which can improve model performance and the ability to generalize. We will use with the following parameters:\n",
    "\n",
    "- `rotation_range`: Degree range for random rotations. We choose 360 degrees since the product is a round object.\n",
    "- `width_shift_range`: Fraction range of the total width to be shifted.\n",
    "- `height_shift_range`: Fraction range of the total height to be shifted.\n",
    "- `shear_range`: Degree range for random shear in a counter-clockwise direction.\n",
    "- `zoom_range`: Fraction range for random zoom.\n",
    "- `horizontal_flip` and `vertical_flip` are set to True for randomly flip image horizontally and vertically.\n",
    "- `brightness_range`: Fraction range for picking a brightness shift value.\n",
    "\n",
    "Other parameters:\n",
    "\n",
    "- `rescale`: Eescale the pixel values to be in range 0 and 1.\n",
    "- `validation_split`: Reserve 20% of the training data for validation, and the rest 80% for model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T12:32:38.811036Z",
     "iopub.status.busy": "2022-06-02T12:32:38.809596Z",
     "iopub.status.idle": "2022-06-02T12:32:38.818722Z",
     "shell.execute_reply": "2022-06-02T12:32:38.817438Z",
     "shell.execute_reply.started": "2022-06-02T12:32:38.81096Z"
    }
   },
   "outputs": [],
   "source": [
    "train_generator = ImageDataGenerator(rotation_range = 360,\n",
    "                                     width_shift_range = 0.05,\n",
    "                                     height_shift_range = 0.05,\n",
    "                                     shear_range = 0.05,\n",
    "                                     zoom_range = 0.05,\n",
    "                                     horizontal_flip = True,\n",
    "                                     vertical_flip = True,\n",
    "                                     brightness_range = [0.75, 1.25],\n",
    "                                     rescale = 1./255,\n",
    "                                     validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATASET PREPARATION**\n",
    "\n",
    "Here is the structure of our folder containing image data:\n",
    "\n",
    "\n",
    "```\n",
    "Tire Texture\n",
    "├───testing_data\n",
    "│   ├───cracked\n",
    "│   └───normal\n",
    "└───training_data\n",
    "    ├───cracked\n",
    "    └───normal\n",
    "```\n",
    "\n",
    "The folder `Tire Texture` consists of two subfolders `testing_data` and `training_data` in which each of them has another subfolder: `cracked` and `normal` denoting the class of our target variable. The images inside `training_data` will be used for model fitting and validation, while `testing_data` will be used purely for testing the model performance on unseen images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define another set of value for the `flow_from_directory` parameters:\n",
    "\n",
    "- `IMAGE_DIR`: The directory where the image data is stored.\n",
    "- `IMAGE_SIZE`: The dimension of the image (379 px by 379 px).\n",
    "- `BATCH_SIZE`: Number of images that will be loaded and trained at one time.\n",
    "- `SEED_NUMBER`: Ensure reproducibility.\n",
    "- `color_mode = \"grayscale\"`: Treat our image with only one channel color.\n",
    "- `class_mode` and `classes` define the target class of our problem. In this case, we denote the `cracked` class as positive (1), and `normal` as a negative class.\n",
    "- `shuffle` = True to make sure the model learns the defect and ok images alternately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T12:32:47.220608Z",
     "iopub.status.busy": "2022-06-02T12:32:47.220149Z",
     "iopub.status.idle": "2022-06-02T12:32:47.965648Z",
     "shell.execute_reply": "2022-06-02T12:32:47.964623Z",
     "shell.execute_reply.started": "2022-06-02T12:32:47.220574Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"../input/tire-texture-image-recognition/Tire Textures/\"\n",
    "\n",
    "IMAGE_SIZE = (379, 379)\n",
    "BATCH_SIZE = 64\n",
    "SEED_NUMBER = 123\n",
    "\n",
    "gen_args = dict(target_size = IMAGE_SIZE,\n",
    "                color_mode = \"grayscale\",\n",
    "                batch_size = BATCH_SIZE,\n",
    "                class_mode = \"binary\",\n",
    "                classes = {\"normal\": 0, \"cracked\": 1},\n",
    "                seed = SEED_NUMBER)\n",
    "\n",
    "train_dataset = train_generator.flow_from_directory(\n",
    "                                        directory = IMAGE_DIR + \"training_data\",\n",
    "                                        subset = \"training\", shuffle = True, **gen_args)\n",
    "validation_dataset = train_generator.flow_from_directory(\n",
    "                                        directory = IMAGE_DIR + \"training_data\",\n",
    "                                        subset = \"validation\", shuffle = True, **gen_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T12:32:51.53884Z",
     "iopub.status.busy": "2022-06-02T12:32:51.53856Z",
     "iopub.status.idle": "2022-06-02T12:32:51.650532Z",
     "shell.execute_reply": "2022-06-02T12:32:51.649682Z",
     "shell.execute_reply.started": "2022-06-02T12:32:51.538811Z"
    }
   },
   "outputs": [],
   "source": [
    "test_generator = ImageDataGenerator(rescale = 1./255)\n",
    "test_dataset = test_generator.flow_from_directory(directory = IMAGE_DIR + \"testing_data\",\n",
    "                                                  shuffle = False,\n",
    "                                                  **gen_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMAGE VISUALIZATION**\n",
    "\n",
    "We successfully load and apply on-the-fly data augmentation according to the specified parameters. Now, in this section, we visualize the image to make sure that it is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T12:32:57.240109Z",
     "iopub.status.busy": "2022-06-02T12:32:57.239549Z",
     "iopub.status.idle": "2022-06-02T12:32:57.249202Z",
     "shell.execute_reply": "2022-06-02T12:32:57.248225Z",
     "shell.execute_reply.started": "2022-06-02T12:32:57.240073Z"
    }
   },
   "outputs": [],
   "source": [
    "mapping_class = {0: \"normal\", 1: \"cracked\"}\n",
    "mapping_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T12:32:59.519945Z",
     "iopub.status.busy": "2022-06-02T12:32:59.519651Z",
     "iopub.status.idle": "2022-06-02T12:32:59.527624Z",
     "shell.execute_reply": "2022-06-02T12:32:59.526662Z",
     "shell.execute_reply.started": "2022-06-02T12:32:59.519915Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualizeImageBatch(dataset, title):\n",
    "    images, labels = next(iter(dataset))\n",
    "    images = images.reshape(BATCH_SIZE, *IMAGE_SIZE)\n",
    "    fig, axes = plt.subplots(8, 8, figsize=(16,16))\n",
    "\n",
    "    for ax, img, label in zip(axes.flat, images, labels):\n",
    "        ax.imshow(img, cmap = \"gray\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(mapping_class[label], size = 20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.suptitle(title, size = 30, y = 1.05, fontweight = \"bold\")\n",
    "    plt.show()\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T12:33:09.45007Z",
     "iopub.status.busy": "2022-06-02T12:33:09.449776Z",
     "iopub.status.idle": "2022-06-02T12:33:20.489473Z",
     "shell.execute_reply": "2022-06-02T12:33:20.488319Z",
     "shell.execute_reply.started": "2022-06-02T12:33:09.45004Z"
    }
   },
   "outputs": [],
   "source": [
    "train_images = visualizeImageBatch(train_dataset,\n",
    "                                   \"FIRST BATCH OF THE TRAINING IMAGES\\n(WITH DATA AUGMENTATION)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T12:34:28.409371Z",
     "iopub.status.busy": "2022-06-02T12:34:28.407635Z",
     "iopub.status.idle": "2022-06-02T12:34:40.815606Z",
     "shell.execute_reply": "2022-06-02T12:34:40.814881Z",
     "shell.execute_reply.started": "2022-06-02T12:34:28.409258Z"
    }
   },
   "outputs": [],
   "source": [
    "test_images = visualizeImageBatch(test_dataset,\n",
    "                                  \"FIRST BATCH OF THE TEST IMAGES\\n(WITHOUT DATA AUGMENTATION)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BUILDING MODEL**\n",
    "As mentioned earlier, we are going to train a CNN model to classify the casting product image. CNN is used as an automatic feature extractor from the images so that it can learn how to distinguish between `defect` and `ok` casted products. It effectively uses the adjacent pixel to downsample the image and then use a prediction (fully-connected) layer to solve the classification problem. This is a simple illustration by [Udacity](https://github.com/udacity/deep-learning-v2-pytorch) on how the layers are arranged sequentially:\n",
    "\n",
    "![](https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/convolutional-neural-networks/conv-visualization/notebook_ims/CNN_all_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Architecture\n",
    "\n",
    "Here is the detailed architecture that we are going to use:\n",
    "\n",
    "1. **First convolutional layer**: consists of 128 filters with kernel_size matrix 3 by 3. Using 2-pixel strides at a time, reduce the image size by half.\n",
    "2. **First pooling layer**: Using max-pooling matrix 2 by 2 (pool_size) and 2-pixel strides at a time further reduce the image size by half.\n",
    "3. **Second convolutional layer**: Just like the first convolutional layer but with 64 filters only.\n",
    "4. **Second pooling layer**: Same as the first pooling layer.\n",
    "5. **Third convolutional layer**: Just like the first and second layer but with 32 filters only.\n",
    "6. **Third pooling layer**: Same as previous pooling layers\n",
    "7. **Forth convolutional layer**: Jusdt like previous layer but with 16 filters only.\n",
    "8. **Forth pooling layer**: Same as previous pooling layers\n",
    "9. **Flattening**: Convert two-dimensional pixel values into one dimension, so that it is ready to be fed into the fully-connected layer.\n",
    "10. **First dense layer + Dropout**: consists of 128 units and 1 bias unit. Dropout of rate 20% is used to prevent overfitting.\n",
    "11. **Second dense layer + Dropout**: consists of 64 units and 1 bias unit. Dropout of rate 20% is also used to prevent overfitting.\n",
    "12. **Output layer**: consists of only one unit and activation is a sigmoid function to convert the scores into a probability of an image being defect.\n",
    "\n",
    "For every layer except output layer, we use Rectified Linear Unit (ReLU) activation function.\n",
    "\n",
    "![relu](https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/convolutional-neural-networks/conv-visualization/notebook_ims/relu_ex.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T12:34:52.5113Z",
     "iopub.status.busy": "2022-06-02T12:34:52.510943Z",
     "iopub.status.idle": "2022-06-02T12:34:52.74517Z",
     "shell.execute_reply": "2022-06-02T12:34:52.743737Z",
     "shell.execute_reply.started": "2022-06-02T12:34:52.511266Z"
    }
   },
   "outputs": [],
   "source": [
    "model_cnn = Sequential(\n",
    "    [\n",
    "        # First convolutional layer\n",
    "        Conv2D(filters = 128,\n",
    "               kernel_size = 3,\n",
    "               strides = 2,\n",
    "               activation = \"relu\",\n",
    "               input_shape = IMAGE_SIZE + (1, )),\n",
    "        \n",
    "        # First pooling layer\n",
    "        MaxPooling2D(pool_size = 2,\n",
    "                     strides = 2),\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        Conv2D(filters = 64,\n",
    "               kernel_size = 3,\n",
    "               strides = 2,\n",
    "               activation = \"relu\"),\n",
    "        \n",
    "        # Second pooling layer\n",
    "        MaxPooling2D(pool_size = 2,\n",
    "                     strides = 2),\n",
    "        \n",
    "        # Third convolutional layer\n",
    "        Conv2D(filters = 32,\n",
    "               kernel_size = 3,\n",
    "               strides = 2,\n",
    "               activation = \"relu\"),\n",
    "        \n",
    "        # Third pooling layer\n",
    "        MaxPooling2D(pool_size = 2,\n",
    "                     strides = 2),\n",
    "        \n",
    "        # Forth convolutional layer\n",
    "        Conv2D(filters = 16,\n",
    "               kernel_size = 3,\n",
    "               strides = 2,\n",
    "               activation = \"relu\"),\n",
    "        \n",
    "        # Forth pooling layer\n",
    "        MaxPooling2D(pool_size = 2,\n",
    "                     strides = 2),\n",
    "        \n",
    "        # Flattening\n",
    "        Flatten(),\n",
    "        \n",
    "        # Fully-connected layer\n",
    "        Dense(128, activation = \"relu\"),\n",
    "        Dropout(rate = 0.2),\n",
    "        \n",
    "        Dense(64, activation = \"relu\"),\n",
    "        Dropout(rate = 0.2),\n",
    "        \n",
    "        Dense(1, activation = \"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model\n",
    "\n",
    "Next, we specify how the model backpropagates or update the weights after each batch feed-forward. We use `adam` optimizer and a loss function `binary cross-entropy` since we are dealing with binary classification problem. The metrics used to monitor the training progress is accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T12:34:59.639173Z",
     "iopub.status.busy": "2022-06-02T12:34:59.638503Z",
     "iopub.status.idle": "2022-06-02T12:34:59.655572Z",
     "shell.execute_reply": "2022-06-02T12:34:59.654702Z",
     "shell.execute_reply.started": "2022-06-02T12:34:59.639113Z"
    }
   },
   "outputs": [],
   "source": [
    "model_cnn.compile(optimizer = 'adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "Before we do model fitting, let's check whether GPU is available or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T12:35:07.715431Z",
     "iopub.status.busy": "2022-06-02T12:35:07.714799Z",
     "iopub.status.idle": "2022-06-02T12:59:07.970352Z",
     "shell.execute_reply": "2022-06-02T12:59:07.969357Z",
     "shell.execute_reply.started": "2022-06-02T12:35:07.715393Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model/cnn_tire_texture_model.hdf5',\n",
    "                             verbose = 1,\n",
    "                             save_best_only = True,\n",
    "                             monitor='val_loss',\n",
    "                             mode='min')\n",
    "\n",
    "model_cnn.fit(train_dataset,\n",
    "                    validation_data = validation_dataset,\n",
    "                    batch_size = 16,\n",
    "                    epochs = 20,\n",
    "                    callbacks = [checkpoint],\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Evaluation\n",
    "Let's plot both loss and accuracy metrics for train and validation data based on each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T12:59:31.573051Z",
     "iopub.status.busy": "2022-06-02T12:59:31.572757Z",
     "iopub.status.idle": "2022-06-02T12:59:31.932493Z",
     "shell.execute_reply": "2022-06-02T12:59:31.931573Z",
     "shell.execute_reply.started": "2022-06-02T12:59:31.573023Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (8, 6))\n",
    "sns.lineplot(data = pd.DataFrame(model_cnn.history.history,\n",
    "                                 index = range(1, 1+len(model_cnn.history.epoch))))\n",
    "plt.title(\"TRAINING EVALUATION\", fontweight = \"bold\", fontsize = 20)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Metrics\")\n",
    "\n",
    "plt.legend(labels = ['val loss', 'val accuracy', 'train loss', 'train accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the model is **not overfitting** the data since both train loss and val loss simultaneously dropped towards zero. Also, both train accuracy and val accuracy increase towards 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Unseen Images\n",
    "\n",
    "Our model performs very well on the training and validation dataset which uses augmented images. Now, we test our model performance with unseen and unaugmented images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T12:59:42.205111Z",
     "iopub.status.busy": "2022-06-02T12:59:42.203917Z",
     "iopub.status.idle": "2022-06-02T12:59:42.358741Z",
     "shell.execute_reply": "2022-06-02T12:59:42.357828Z",
     "shell.execute_reply.started": "2022-06-02T12:59:42.205041Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model = load_model(\"model/cnn_tire_texture_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T12:59:47.074514Z",
     "iopub.status.busy": "2022-06-02T12:59:47.074195Z",
     "iopub.status.idle": "2022-06-02T13:00:18.492573Z",
     "shell.execute_reply": "2022-06-02T13:00:18.491672Z",
     "shell.execute_reply.started": "2022-06-02T12:59:47.07448Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_prob = best_model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the prediction is in the form of probability. We use THRESHOLD = 0.5 to separate the classes. If the probability is greater or equal to the THRESHOLD, then it will be classified as cracked, otherwise normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T13:00:55.020504Z",
     "iopub.status.busy": "2022-06-02T13:00:55.020188Z",
     "iopub.status.idle": "2022-06-02T13:00:55.045842Z",
     "shell.execute_reply": "2022-06-02T13:00:55.044971Z",
     "shell.execute_reply.started": "2022-06-02T13:00:55.020465Z"
    }
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "y_pred_class = (y_pred_prob >= THRESHOLD).reshape(-1,)\n",
    "y_true_class = test_dataset.classes[test_dataset.index_array]\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(y_true_class, y_pred_class),\n",
    "    index = [[\"Actual\", \"Actual\"], [\"normal\", \"cracked\"]],\n",
    "    columns = [[\"Predicted\", \"Predicted\"], [\"normal\", \"cracked\"]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T13:01:02.992543Z",
     "iopub.status.busy": "2022-06-02T13:01:02.991924Z",
     "iopub.status.idle": "2022-06-02T13:01:03.004851Z",
     "shell.execute_reply": "2022-06-02T13:01:03.003478Z",
     "shell.execute_reply.started": "2022-06-02T13:01:02.992495Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true_class, y_pred_class, digits = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the problem statement, we want to minimize the case of False Negative, where the defect product is misclassified as `normal`. This can cause the whole order to be rejected and create a big loss for the company. Therefore, in this case, we prioritize Recall over Precision.\n",
    "\n",
    "But if we take into account the cost of re-casting a product, we have to minimize the case of False Positive also, where the normal product is misclassified as `cracked`. Therefore we can prioritize the `F1 score` which combines both Recall and Precision.\n",
    "\n",
    "On test dataset, the model achieves not so good result as follow:\n",
    "\n",
    "- Accuracy: 55.84%\n",
    "- Recall: 61.90%\n",
    "- Precision: 70.27%\n",
    "- F1 score: 65.82%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "By using CNN and on-the-fly data augmentation, the performance of our model in training, validation, and test images is not good, reaching 58-65% accuracy and F1 score. We can utilize this model by embedding it into a surveillance camera where the system can automatically separate defective product from the production line. This method surely can reduce human error and human resources on manual inspection, but it still needs supervision from human since the model is not 100% correct at all times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8ef873d6a9fb09128c51ca24f57d685a29a4c4cb4919f17482f419e89ebc151a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
